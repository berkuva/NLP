{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first got rid of all special characters using Unix command:\n",
    "sed \"s/[^a-zA-Z]/ /g\" training_data.txt > new_training_data.txt.\n",
    "My new_training_data.txt only contains alphabet letters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (new) training data and labels\n",
    "\n",
    "train_datafile = open(\"./src/data_files/new_training_data.txt\")\n",
    "train_datafile_lines = [line.rstrip('\\n') for line in train_datafile]\n",
    "train_labels = np.loadtxt(\"./src/data_files/training_labels.txt\",\\\n",
    "                          delimiter='\\n', dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  40924\n",
      "(30000, 40924)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = text.CountVectorizer()\n",
    "\n",
    "training_data = vectorizer.fit_transform(train_datafile_lines)\n",
    "words = vectorizer.vocabulary_\n",
    "print(\"vocabulary size: \", len(words))\n",
    "print(training_data.shape)\n",
    "\n",
    "\"\"\"\n",
    "The size of my feature set: 40924\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40924\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "\n",
    "val_datafile = open(\"./src/data_files/val_data.txt\")\n",
    "val_datafile_lines = [line.rstrip('\\n') for line in val_datafile]\n",
    "\n",
    "val_vectorizer = text.CountVectorizer(vocabulary=words)\n",
    "\n",
    "val_data = val_vectorizer.fit_transform(val_datafile_lines)\n",
    "val_labels = np.loadtxt(\"./src/data_files/val_labels.txt\", delimiter='\\n', dtype=np.int32)\n",
    "# print(len(val_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.fit(training_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 0.9754333333333334\n",
      "Validation accuracy = 0.8802\n"
     ]
    }
   ],
   "source": [
    "# Measure the performance on training and validation data \n",
    "print(\"Training accuracy =\", logistic_regression.score(training_data, train_labels))\n",
    "print(\"Validation accuracy =\", logistic_regression.score(val_data, val_labels))\n",
    "\n",
    "'''\n",
    "Training accuracy = 0.9754333333333334\n",
    "Validation accuracy = 0.8802\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the feature set is the number of words in my word set, which is 40924.\n",
    "Training accuracy = 0.9754333333333334\n",
    "Validation accuracy = 0.8802."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-train => change ngram_range from (1,1) to (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorizer2 = text.CountVectorizer(ngram_range=(1,2))\n",
    "train_data2 = train_vectorizer2.fit_transform(train_datafile_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  736981\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size: \", len(train_vectorizer2.vocabulary_))\n",
    "\n",
    "\"\"\"\n",
    "The size of my feature set after changing ngram_range from (1,1) to (1,2):  736981\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression2 = LogisticRegression()\n",
    "logistic_regression2.fit(train_data2, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_vectorizer2 = text.CountVectorizer(vocabulary=train_vectorizer2.vocabulary_)\n",
    "val_data2 = val_vectorizer2.fit_transform(val_datafile_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy with ngram_range (1,2) = 0.9999\n",
      "Validation accuracy with ngram_range (1,2) = 0.8763\n"
     ]
    }
   ],
   "source": [
    "# Measure the performance on training and validation data \n",
    "print(\"Training accuracy with ngram_range (1,2) =\",\\\n",
    "      logistic_regression2.score(train_data2, train_labels))\n",
    "print(\"Validation accuracy with ngram_range (1,2) =\",\\\n",
    "      logistic_regression2.score(val_data2, val_labels))\n",
    "\n",
    "\"\"\"\n",
    "Training accuracy with ngram_range (1,2) = 0.9999\n",
    "Validation accuracy with ngram_range (1,2) = 0.8763\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the feature set is the number of words in my word set, which is 736981.\n",
    "Training accuracy = 0.9999\n",
    "Validation accuracy = 0.8763. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default value of C in sklearn LogisticRegression is 1.0, so the lambda = 1/C = 1.0. \n",
    "\n",
    "C = 1/lambda. try lambda = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy [penalty=L2, ngram_range=(1,2), C=10000.0] = 1.0\n",
      "Validation accuracy [penalty=L2, ngram_range=(1,2), C=10000.0] = 0.8603\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L2, ngram_range=(1,2), C=1000.0] = 1.0\n",
      "Validation accuracy [penalty=L2, ngram_range=(1,2), C=1000.0] = 0.8634\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L2, ngram_range=(1,2), C=100.0] = 1.0\n",
      "Validation accuracy [penalty=L2, ngram_range=(1,2), C=100.0] = 0.8705\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L2, ngram_range=(1,2), C=10.0] = 1.0\n",
      "Validation accuracy [penalty=L2, ngram_range=(1,2), C=10.0] = 0.8727\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L2, ngram_range=(1,2), C=1.0] = 0.9999\n",
      "Validation accuracy [penalty=L2, ngram_range=(1,2), C=1.0] = 0.8763\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L2, ngram_range=(1,2), C=0.1] = 0.9932\n",
      "Validation accuracy [penalty=L2, ngram_range=(1,2), C=0.1] = 0.8772\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L2, ngram_range=(1,2), C=0.01] = 0.9431333333333334\n",
      "Validation accuracy [penalty=L2, ngram_range=(1,2), C=0.01] = 0.8696\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L2, ngram_range=(1,2), C=0.001] = 0.8763\n",
      "Validation accuracy [penalty=L2, ngram_range=(1,2), C=0.001] = 0.8384\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L2, ngram_range=(1,2), C=0.0001] = 0.8115\n",
      "Validation accuracy [penalty=L2, ngram_range=(1,2), C=0.0001] = 0.7864\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambdas = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000]\n",
    "for lamb in lambdas:\n",
    "    c = 1/lamb\n",
    "    classifier = LogisticRegression(C=c)\n",
    "    classifier.fit(train_data2, train_labels)\n",
    "    \n",
    "    # Measure the performance on training and validation data \n",
    "    print(\"Training accuracy [penalty=L2, ngram_range=(1,2), C={}] =\".format(c),\\\n",
    "          classifier.score(train_data2, train_labels))\n",
    "    print(\"Validation accuracy [penalty=L2, ngram_range=(1,2), C={}] =\".format(c),\\\n",
    "          classifier.score(val_data2, val_labels))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "Training accuracy [penalty=L2, ngram_range=(1,2), C=10000.0] = 1.0\n",
    "Validation accuracy [penalty=L2, ngram_range=(1,2), C=10000.0] = 0.8603\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L2, ngram_range=(1,2), C=1000.0] = 1.0\n",
    "Validation accuracy [penalty=L2, ngram_range=(1,2), C=1000.0] = 0.8634\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L2, ngram_range=(1,2), C=100.0] = 1.0\n",
    "Validation accuracy [penalty=L2, ngram_range=(1,2), C=100.0] = 0.8705\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L2, ngram_range=(1,2), C=10.0] = 1.0\n",
    "Validation accuracy [penalty=L2, ngram_range=(1,2), C=10.0] = 0.8727\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L2, ngram_range=(1,2), C=1.0] = 0.9999\n",
    "Validation accuracy [penalty=L2, ngram_range=(1,2), C=1.0] = 0.8763\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L2, ngram_range=(1,2), C=0.1] = 0.9932\n",
    "Validation accuracy [penalty=L2, ngram_range=(1,2), C=0.1] = 0.8772\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L2, ngram_range=(1,2), C=0.01] = 0.9431333333333334\n",
    "Validation accuracy [penalty=L2, ngram_range=(1,2), C=0.01] = 0.8696\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L2, ngram_range=(1,2), C=0.001] = 0.8763\n",
    "Validation accuracy [penalty=L2, ngram_range=(1,2), C=0.001] = 0.8384\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L2, ngram_range=(1,2), C=0.0001] = 0.8115\n",
    "Validation accuracy [penalty=L2, ngram_range=(1,2), C=0.0001] = 0.7864\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: The best validation accuracy came with C=0.1 with 0.8772."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since applying L1-regularization to the weight vector theta is defined by the sum of the absolute value of the elements in the vector, plotting theta in the coordinate system results in a diamond shape in #dimensions where # is the number of elements in theta. Before applying L1-regularization, the optimal value of the objective function sits somewhere in the coordinate. By moving away from that point by following the contour plots, it is highly likely that the contour plots hit the axes on which the diamond's angles sit. This means that those weights get set to 0, resulting in the sparsity of theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy [penalty=L1, ngram_range=(1,2), C=10000.0] = 1.0\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=10000.0] = 0.8468\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L1, ngram_range=(1,2), C=1000.0] = 1.0\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=1000.0] = 0.8668\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L1, ngram_range=(1,2), C=100.0] = 1.0\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=100.0] = 0.8719\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L1, ngram_range=(1,2), C=10.0] = 1.0\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=10.0] = 0.8646\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L1, ngram_range=(1,2), C=1.0] = 0.9913\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=1.0] = 0.8685\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L1, ngram_range=(1,2), C=0.1] = 0.9111\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.1] = 0.8723\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L1, ngram_range=(1,2), C=0.01] = 0.8334666666666667\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.01] = 0.8204\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L1, ngram_range=(1,2), C=0.001] = 0.681\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.001] = 0.6725\n",
      "\n",
      "\n",
      "Training accuracy [penalty=L1, ngram_range=(1,2), C=0.0001] = 0.4987666666666667\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.0001] = 0.5013\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambdas = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 10000]\n",
    "for lamb in lambdas:\n",
    "    c = 1/lamb\n",
    "    classifier2 = LogisticRegression(penalty='l1', C=c)\n",
    "    classifier2.fit(train_data2, train_labels)\n",
    "    \n",
    "    # Measure the performance on training and validation data \n",
    "    print(\"Training accuracy [penalty=L1, ngram_range=(1,2), C={}] =\".format(c),\\\n",
    "          classifier2.score(train_data2, train_labels))\n",
    "    print(\"Validation accuracy [penalty=L1, ngram_range=(1,2), C={}] =\".format(c),\\\n",
    "          classifier2.score(val_data2, val_labels))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training accuracy [penalty=L1, ngram_range=(1,2), C=10000.0] = 1.0\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=10000.0] = 0.8468\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L1, ngram_range=(1,2), C=1000.0] = 1.0\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=1000.0] = 0.8668\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L1, ngram_range=(1,2), C=100.0] = 1.0\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=100.0] = 0.8719\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L1, ngram_range=(1,2), C=10.0] = 1.0\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=10.0] = 0.8646\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L1, ngram_range=(1,2), C=1.0] = 0.9913\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=1.0] = 0.8685\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L1, ngram_range=(1,2), C=0.1] = 0.9111\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.1] = 0.8723\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L1, ngram_range=(1,2), C=0.01] = 0.8334666666666667\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.01] = 0.8204\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L1, ngram_range=(1,2), C=0.001] = 0.681\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.001] = 0.6725\n",
    "\n",
    "\n",
    "Training accuracy [penalty=L1, ngram_range=(1,2), C=0.0001] = 0.4987666666666667\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.0001] = 0.5013\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: The best validation accuracy came with C=0.1 with 0.8723."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in the validation accuracy of ngram_range = (1,1) and ngram_range = (1,2) was very small. Similarly, there is not a clear winner between L1 and L2 regularizations since the validation accuracies are very close. However, in both cases, the best validation accuracy was observed when C=0.1 (lambda=10). SGDClassifier with loss='log' is a logistic regression model. Using this model allows elasticnet penalty, which mixes L2 and L1 regularizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=1e-05] = 0.971\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=1e-05] = 0.8524\n",
      "\n",
      "\n",
      "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=0.0001] = 0.9707333333333333\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.0001] = 0.8596\n",
      "\n",
      "\n",
      "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=0.0001] = 0.9687333333333333\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.0001] = 0.8604\n",
      "\n",
      "\n",
      "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=0.001] = 0.8767\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.001] = 0.8102\n",
      "\n",
      "\n",
      "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=0.01] = 0.8416333333333333\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.01] = 0.83\n",
      "\n",
      "\n",
      "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=0.1] = 0.7230333333333333\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.1] = 0.7194\n",
      "\n",
      "\n",
      "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=1] = 0.5012333333333333\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=1] = 0.4987\n",
      "\n",
      "\n",
      "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=10] = 0.5012333333333333\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=10] = 0.4987\n",
      "\n",
      "\n",
      "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=100] = 0.5012333333333333\n",
      "Validation accuracy [penalty=L1, ngram_range=(1,2), C=100] = 0.4987\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "alphas = [1e-5, 1e-4, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "for al in alphas:\n",
    "    SGD_classifier = SGDClassifier(loss='log', penalty='elasticnet', alpha=al)\n",
    "    SGD_classifier.fit(train_data2, train_labels)\n",
    "    \n",
    "    # Measure the performance on training and validation data \n",
    "    print(\"Training accuracy [penalty=elasticnet, ngram_range=(1,2), C={}] =\".format(al),\\\n",
    "          SGD_classifier.score(train_data2, train_labels))\n",
    "    print(\"Validation accuracy [penalty=L1, ngram_range=(1,2), C={}] =\".format(al),\\\n",
    "          SGD_classifier.score(val_data2, val_labels))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=1e-05] = 0.971\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=1e-05] = 0.8524\n",
    "\n",
    "\n",
    "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=0.0001] = 0.9707333333333333\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.0001] = 0.8596\n",
    "\n",
    "\n",
    "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=0.0001] = 0.9687333333333333\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.0001] = 0.8604\n",
    "\n",
    "\n",
    "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=0.001] = 0.8767\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.001] = 0.8102\n",
    "\n",
    "\n",
    "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=0.01] = 0.8416333333333333\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.01] = 0.83\n",
    "\n",
    "\n",
    "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=0.1] = 0.7230333333333333\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=0.1] = 0.7194\n",
    "\n",
    "\n",
    "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=1] = 0.5012333333333333\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=1] = 0.4987\n",
    "\n",
    "\n",
    "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=10] = 0.5012333333333333\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=10] = 0.4987\n",
    "\n",
    "\n",
    "Training accuracy [penalty=elasticnet, ngram_range=(1,2), C=100] = 0.5012333333333333\n",
    "Validation accuracy [penalty=L1, ngram_range=(1,2), C=100] = 0.4987\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: The best validation accuracy of (0.8671) is achieved with C=0.001, but this is very close to the above models and their parameters. Since some models have very similar accuracies, let's try Cross-Validation with f1 and accuracy scoring metrics to determine the best model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validate, [scoring = f1, ngram_range(1,1)] =  [0.87653587 0.87782178 0.8803334 ]\n",
      "cross_validate, [scoring = accuracy, ngram_range(1,1)] =  [0.87541246 0.8766     0.87938794]\n"
     ]
    }
   ],
   "source": [
    "# ngram_range(1,1)\n",
    "cv_1 = cross_validate(logistic_regression, training_data, train_labels,\\\n",
    "                      scoring=('f1', 'accuracy'))\n",
    "print(\"cross_validate, [scoring = f1, ngram_range(1,1)] = \", cv_1['test_f1'])\n",
    "print(\"cross_validate, [scoring = accuracy, ngram_range(1,1)] = \", cv_1['test_accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "cross_validate, [scoring = f1, ngram_range(1,1)] =  [0.87653587 0.87782178 0.8803334 ]\n",
    "cross_validate, [scoring = accuracy, ngram_range(1,1)] =  [0.87541246 0.8766     0.87938794]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validate, [scoring = f1, ngram_range(1,2)] =  [0.87653587 0.87782178 0.8803334 ]\n",
      "cross_validate, [scoring = accuracy, ngram_range(1,2)] =  [0.87541246 0.8766     0.87938794]\n"
     ]
    }
   ],
   "source": [
    "# ngram_range(1,2)\n",
    "\n",
    "cv_2 = cross_validate(logistic_regression2, training_data, train_labels,\\\n",
    "                      scoring=('f1', 'accuracy'))\n",
    "print(\"cross_validate, [scoring = f1, ngram_range(1,2)] = \", cv_2['test_f1'])\n",
    "print(\"cross_validate, [scoring = accuracy, ngram_range(1,2)] = \", cv_2['test_accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "cross_validate, [scoring = f1, ngram_range(1,2)] =  [0.87653587 0.87782178 0.8803334 ]\n",
    "cross_validate, [scoring = accuracy, ngram_range(1,2)] =  [0.87541246 0.8766     0.87938794]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validate, [scoring = f1, ngram_range(1,2)] =  [0.880976   0.8834817  0.88453547]\n",
      "cross_validate, [scoring = accuracy, ngram_range(1,2)] =  [0.880012   0.8822     0.88328833]\n"
     ]
    }
   ],
   "source": [
    "# L2 regularization, ngram_range(1,2), C=0.1\n",
    "\n",
    "c3 = LogisticRegression(C=0.1).fit(train_data2, train_labels)\n",
    "cv_3 = cross_validate(c3, training_data, train_labels, scoring=('f1', 'accuracy'))\n",
    "print(\"cross_validate, [scoring = f1, ngram_range(1,2)] = \", cv_3['test_f1'])\n",
    "print(\"cross_validate, [scoring = accuracy, ngram_range(1,2)] = \", cv_3['test_accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "cross_validate, [scoring = f1, ngram_range(1,2)] =  [0.880976   0.8834817  0.88453547]\n",
    "cross_validate, [scoring = accuracy, ngram_range(1,2)] =  [0.880012   0.8822     0.88328833]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validate, [scoring = f1, ngram_range(1,2)] =  [0.87023656 0.87391304 0.8756917 ]\n",
      "cross_validate, [scoring = accuracy, ngram_range(1,2)] =  [0.86891311 0.8724     0.87418742]\n"
     ]
    }
   ],
   "source": [
    "# L1 regularization, ngram_range(1,2), C=0.1\n",
    "c4 = LogisticRegression(penalty='l1', C=0.1).fit(train_data2, train_labels)\n",
    "cv_4 = cross_validate(c4, training_data, train_labels, scoring=('f1', 'accuracy'))\n",
    "print(\"cross_validate, [scoring = f1, ngram_range(1,2)] = \", cv_4['test_f1'])\n",
    "print(\"cross_validate, [scoring = accuracy, ngram_range(1,2)] = \", cv_4['test_accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "cross_validate, [scoring = f1, ngram_range(1,2)] =  [0.87023656 0.87391304 0.8756917 ]\n",
    "cross_validate, [scoring = accuracy, ngram_range(1,2)] =  [0.86891311 0.8724     0.87418742]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validate, [scoring = f1, ngram_range(1,2)] =  [0.87587685 0.87362137 0.87384191]\n",
      "cross_validate, [scoring = accuracy, ngram_range(1,2)] =  [0.87261274 0.8751     0.8719872 ]\n"
     ]
    }
   ],
   "source": [
    "# Elastic Net regularization, ngram_range(1,2), C=0.001\n",
    "c5 = SGDClassifier(loss='log', penalty='elasticnet', alpha=0.001).fit(train_data2, train_labels)\n",
    "cv_5 = cross_validate(c5, training_data, train_labels, scoring=('f1', 'accuracy'))\n",
    "print(\"cross_validate, [scoring = f1, ngram_range(1,2)] = \", cv_5['test_f1'])\n",
    "print(\"cross_validate, [scoring = accuracy, ngram_range(1,2)] = \", cv_5['test_accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "cross_validate, [scoring = f1, ngram_range(1,2)] =  [0.87587685 0.87362137 0.87384191]\n",
    "cross_validate, [scoring = accuracy, ngram_range(1,2)] =  [0.87261274 0.8751     0.8719872 ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like using the F1 metric yields slightly better scores, and the best cross_validation score achieved was (cv_3) L2 regularization, ngram_range(1,2), C=0.1 with the mean F1 score of 0.883. So, I will use (cv_3) to make predictions for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, L2 regularization, ngram_range(1,2), C=0.1 yields training accuracy of 0.9932 and validation accuracy of 0.8772."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datafile = open(\"./src/data_files/test_data.txt\")\n",
    "test_datafile_lines = [line.rstrip('\\n') for line in test_datafile]\n",
    "\n",
    "test_vectorizer = text.CountVectorizer(vocabulary=train_vectorizer2.vocabulary_)\n",
    "test_data = test_vectorizer.fit_transform(test_datafile_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = c3.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"hc2kc-lr-test.pred\", predictions.astype(int), fmt='%.0f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
